{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이 소스코드는 pyspark를 이용하여 로우 데이터를 학습에 적합한 피쳐와 라벨이 있는 데이터셋으로 변환\n",
    "#파이프라인을 만들고 spark의 ML패키지를 이용하여 학습한 코드입니다.\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window\n",
    "from pyspark.sql.functions import col, column, count, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load('C:/Users/Shin/Documents/testbeddata/sensor/20190417_sensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: timestamp (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(col(\"_c0\").alias(\"timestamp\"), col(\"_c1\").alias(\"sensor\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WindowedCount = df.groupBy(window(df.timestamp,'10 seconds','1 seconds')).\\\n",
    "                agg(\n",
    "                count(when(col('sensor')=='LR0 On', True)).alias('LR0'),\\\n",
    "                count(when(col('sensor')=='LR1 On', True)).alias('LR1'),\\\n",
    "                count(when(col('sensor')=='LR2 On', True)).alias('LR2'),\\\n",
    "                count(when(col('sensor')=='LR3 On', True)).alias('LR3'),\\\n",
    "                count(when(col('sensor')=='LR4 On', True)).alias('LR4'),\\\n",
    "                count(when(col('sensor')=='LR5 On', True)).alias('LR5'),\\\n",
    "                count(when(col('sensor')=='LR6 On', True)).alias('LR6'),\\\n",
    "                count(when(col('sensor')=='LR7 On', True)).alias('LR7'),\\\n",
    "                count(when(col('sensor')=='LR8 On', True)).alias('LR8'),\\\n",
    "                count(when(col('sensor')=='LR9 On', True)).alias('LR9'),\\\n",
    "                count(when(col('sensor')=='LR10 On', True)).alias('LR10'),\\\n",
    "                count(when(col('sensor')=='LR11 On', True)).alias('LR11'),\\\n",
    "                count(when(col('sensor')=='LR12 On', True)).alias('LR12'),\\\n",
    "                count(when(col('sensor')=='LR13 On', True)).alias('LR13'),\\\n",
    "                count(when(col('sensor')=='LR14 On', True)).alias('LR14'),\\\n",
    "                count(when(col('sensor')=='LP0 On', True)).alias('LP0'),\\\n",
    "                count(when(col('sensor')=='LP1 On', True)).alias('LP1'),\\\n",
    "                count(when(col('sensor')=='LP2 On', True)).alias('LP2'),\\\n",
    "                count(when(col('sensor')=='LD On', True)).alias('LD'),\\\n",
    "                count(when(col('sensor')=='BeR0 On', True)).alias('BeR0'),\\\n",
    "                count(when(col('sensor')=='BeR1 On', True)).alias('BeR1'),\\\n",
    "                count(when(col('sensor')=='BeR2 On', True)).alias('BeR2'),\\\n",
    "                count(when(col('sensor')=='BeR3 On', True)).alias('BeR3'),\\\n",
    "                count(when(col('sensor')=='BeR4 On', True)).alias('BeR4'),\\\n",
    "                count(when(col('sensor')=='BeR5 On', True)).alias('BeR5'),\\\n",
    "                count(when(col('sensor')=='BeR6 On', True)).alias('BeR6'),\\\n",
    "                count(when(col('sensor')=='BeR7 On', True)).alias('BeR7'),\\\n",
    "                count(when(col('sensor')=='BeR8 On', True)).alias('BeR8'),\\\n",
    "                count(when(col('sensor')=='BeP0 On', True)).alias('BeP0'),\\\n",
    "                count(when(col('sensor')=='BeP1 On', True)).alias('BeP1'),\\\n",
    "                count(when(col('sensor')=='BeP2 On', True)).alias('BeP2'),\\\n",
    "                count(when(col('sensor')=='BeD On', True)).alias('BeD'),\\\n",
    "                count(when(col('sensor')=='BathR0 On', True)).alias('BathR0'),\\\n",
    "                count(when(col('sensor')=='BathR1 On', True)).alias('BathR1'),\\\n",
    "                count(when(col('sensor')=='BathR2 On', True)).alias('BathR2'),\\\n",
    "                count(when(col('sensor')=='BathP0 On', True)).alias('BathP0'),\\\n",
    "                count(when(col('sensor')=='BathD On', True)).alias('BathD'),\\\n",
    "                ).orderBy('window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- LR0: long (nullable = false)\n",
      " |-- LR1: long (nullable = false)\n",
      " |-- LR2: long (nullable = false)\n",
      " |-- LR3: long (nullable = false)\n",
      " |-- LR4: long (nullable = false)\n",
      " |-- LR5: long (nullable = false)\n",
      " |-- LR6: long (nullable = false)\n",
      " |-- LR7: long (nullable = false)\n",
      " |-- LR8: long (nullable = false)\n",
      " |-- LR9: long (nullable = false)\n",
      " |-- LR10: long (nullable = false)\n",
      " |-- LR11: long (nullable = false)\n",
      " |-- LR12: long (nullable = false)\n",
      " |-- LR13: long (nullable = false)\n",
      " |-- LR14: long (nullable = false)\n",
      " |-- LP0: long (nullable = false)\n",
      " |-- LP1: long (nullable = false)\n",
      " |-- LP2: long (nullable = false)\n",
      " |-- LD: long (nullable = false)\n",
      " |-- BeR0: long (nullable = false)\n",
      " |-- BeR1: long (nullable = false)\n",
      " |-- BeR2: long (nullable = false)\n",
      " |-- BeR3: long (nullable = false)\n",
      " |-- BeR4: long (nullable = false)\n",
      " |-- BeR5: long (nullable = false)\n",
      " |-- BeR6: long (nullable = false)\n",
      " |-- BeR7: long (nullable = false)\n",
      " |-- BeR8: long (nullable = false)\n",
      " |-- BeP0: long (nullable = false)\n",
      " |-- BeP1: long (nullable = false)\n",
      " |-- BeP2: long (nullable = false)\n",
      " |-- BeD: long (nullable = false)\n",
      " |-- BathR0: long (nullable = false)\n",
      " |-- BathR1: long (nullable = false)\n",
      " |-- BathR2: long (nullable = false)\n",
      " |-- BathP0: long (nullable = false)\n",
      " |-- BathD: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WindowedCount.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WindowedCount = WindowedCount.withColumn(\"timestamp\", WindowedCount[\"window.end\"])\n",
    "WindowedCount = WindowedCount.drop('window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LR0: long (nullable = false)\n",
      " |-- LR1: long (nullable = false)\n",
      " |-- LR2: long (nullable = false)\n",
      " |-- LR3: long (nullable = false)\n",
      " |-- LR4: long (nullable = false)\n",
      " |-- LR5: long (nullable = false)\n",
      " |-- LR6: long (nullable = false)\n",
      " |-- LR7: long (nullable = false)\n",
      " |-- LR8: long (nullable = false)\n",
      " |-- LR9: long (nullable = false)\n",
      " |-- LR10: long (nullable = false)\n",
      " |-- LR11: long (nullable = false)\n",
      " |-- LR12: long (nullable = false)\n",
      " |-- LR13: long (nullable = false)\n",
      " |-- LR14: long (nullable = false)\n",
      " |-- LP0: long (nullable = false)\n",
      " |-- LP1: long (nullable = false)\n",
      " |-- LP2: long (nullable = false)\n",
      " |-- LD: long (nullable = false)\n",
      " |-- BeR0: long (nullable = false)\n",
      " |-- BeR1: long (nullable = false)\n",
      " |-- BeR2: long (nullable = false)\n",
      " |-- BeR3: long (nullable = false)\n",
      " |-- BeR4: long (nullable = false)\n",
      " |-- BeR5: long (nullable = false)\n",
      " |-- BeR6: long (nullable = false)\n",
      " |-- BeR7: long (nullable = false)\n",
      " |-- BeR8: long (nullable = false)\n",
      " |-- BeP0: long (nullable = false)\n",
      " |-- BeP1: long (nullable = false)\n",
      " |-- BeP2: long (nullable = false)\n",
      " |-- BeD: long (nullable = false)\n",
      " |-- BathR0: long (nullable = false)\n",
      " |-- BathR1: long (nullable = false)\n",
      " |-- BathR2: long (nullable = false)\n",
      " |-- BathP0: long (nullable = false)\n",
      " |-- BathD: long (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WindowedCount.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = spark.read.format('csv')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option(\"timestampFormat\",\"yyyy.MM.dd HH:mm:ss\")\\\n",
    "    .load('C:/Users/Shin/Documents/testbeddata/servay/20190417.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = sv.first()['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "endtime = sv.orderBy('timestamp', ascending=False).first()['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "step = datetime.timedelta(seconds=1)\n",
    "timestamp = []\n",
    "\n",
    "while starttime < endtime:\n",
    "    timestamp.append(starttime)\n",
    "    starttime += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svtime = spark.createDataFrame(timestamp,'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svtime.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svtime = svtime.select(col(\"value\").alias(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Survey = svtime.join(sv, svtime.timestamp == sv.timestamp, how='left').drop(sv.timestamp) # Could also use 'left_outer'\n",
    "#Survey.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "Survey = Survey.withColumn('session', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "Survey = Survey.withColumn(\"state\", func.last('state', True).over(Window.partitionBy('session').orderBy('timestamp').rowsBetween(-sys.maxsize, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "cols = split(Survey['state'], ' ')\n",
    "Survey = Survey.withColumn('nstate', cols.getItem(0))\n",
    "Survey = Survey.withColumn('column2', cols.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Survey = Survey.filter(Survey.column2 == 'start').drop(Survey.session).drop(Survey.state).drop(Survey.column2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Survey.join(WindowedCount, Survey.timestamp == WindowedCount.timestamp, how='left').drop(WindowedCount.timestamp).drop(Survey.timestamp) # Could also use 'left_outer'\n",
    "dataset = dataset.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nstate: string (nullable = true)\n",
      " |-- LR0: long (nullable = true)\n",
      " |-- LR1: long (nullable = true)\n",
      " |-- LR2: long (nullable = true)\n",
      " |-- LR3: long (nullable = true)\n",
      " |-- LR4: long (nullable = true)\n",
      " |-- LR5: long (nullable = true)\n",
      " |-- LR6: long (nullable = true)\n",
      " |-- LR7: long (nullable = true)\n",
      " |-- LR8: long (nullable = true)\n",
      " |-- LR9: long (nullable = true)\n",
      " |-- LR10: long (nullable = true)\n",
      " |-- LR11: long (nullable = true)\n",
      " |-- LR12: long (nullable = true)\n",
      " |-- LR13: long (nullable = true)\n",
      " |-- LR14: long (nullable = true)\n",
      " |-- LP0: long (nullable = true)\n",
      " |-- LP1: long (nullable = true)\n",
      " |-- LP2: long (nullable = true)\n",
      " |-- LD: long (nullable = true)\n",
      " |-- BeR0: long (nullable = true)\n",
      " |-- BeR1: long (nullable = true)\n",
      " |-- BeR2: long (nullable = true)\n",
      " |-- BeR3: long (nullable = true)\n",
      " |-- BeR4: long (nullable = true)\n",
      " |-- BeR5: long (nullable = true)\n",
      " |-- BeR6: long (nullable = true)\n",
      " |-- BeR7: long (nullable = true)\n",
      " |-- BeR8: long (nullable = true)\n",
      " |-- BeP0: long (nullable = true)\n",
      " |-- BeP1: long (nullable = true)\n",
      " |-- BeP2: long (nullable = true)\n",
      " |-- BeD: long (nullable = true)\n",
      " |-- BathR0: long (nullable = true)\n",
      " |-- BathR1: long (nullable = true)\n",
      " |-- BathR2: long (nullable = true)\n",
      " |-- BathP0: long (nullable = true)\n",
      " |-- BathD: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, RFormula\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"nstate\", outputCol=\"indexedLabel\").fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = RFormula(\n",
    "    formula=\"~.-nstate\",\n",
    "    featuresCol=\"features\").fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer, formula, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------------------+\n",
      "|predictedLabel|nstate|            features|\n",
      "+--------------+------+--------------------+\n",
      "|          bath|  bath|     (37,[33],[1.0])|\n",
      "|          bath|  bath|(37,[9,11,12,32,3...|\n",
      "|          cook|  cook|(37,[0,1,2,3,4,5,...|\n",
      "|           eat|   eat|(37,[1,2,3,5],[1....|\n",
      "|          cook|   eat|(37,[0,1,2,3,4,5,...|\n",
      "+--------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.102329\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_f71df124c9f3) with 200 trees\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"nstate\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
